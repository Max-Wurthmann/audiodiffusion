from typing import Optional, Sequence, Union, List

import torch
from torch import Tensor, nn, optim
from ema_pytorch import EMA
from audio_diffusion_pytorch import DiffusionModel, DiffusionAE, UNetV0, VDiffusion, VSampler
import pytorch_lightning as pl
from transformers import AutoTokenizer, AutoModel


class CombinedSampler:
    """ combines diffae and diffgen to a text-to-audio diffusion model """
    
    def __init__(
        self,
        diffae: DiffusionAE,
        diffgen: DiffusionModel,
        text_encoder_max_length: int,
        device: Union[str, torch.device], 
        text_encoder_name: Optional[str] = "t5-base", 
        num_latent_channels: Optional[int] = 32,
    ):
        """ see also: get_default_combined_sampler """
    
        self.device = device
        self.num_latent_channels = num_latent_channels
        
        # diffusion autoencoder
        self.diffae = diffae.eval().to(self.device)

        # diffusion generator
        self.diffgen = diffgen.eval().to(self.device)

        self.text_encoder_name = text_encoder_name
        self.text_encoder_max_length = text_encoder_max_length
        # Tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(self.text_encoder_name, model_max_length=self.text_encoder_max_length)
        # only the encoder part of the transformer in eval mode on the correct device
        self.text_encoder = AutoModel.from_pretrained(self.text_encoder_name).encoder
        self.text_encoder.eval().to(self.device)
        
    @torch.no_grad()
    def get_text_embeddings(self, texts: List[str]) -> Tensor:
        # Compute batch of tokens and mask from texts
        tokenized = self.tokenizer.batch_encode_plus(
            texts,
            return_tensors="pt",
            padding="max_length",
            max_length=self.text_encoder_max_length,
            truncation=True,
        )
        ids, mask = tokenized["input_ids"], tokenized["attention_mask"]
        
        # Encode with transformer
        embeddings = self.text_encoder(
            input_ids=ids.to(self.device),
            attention_mask=mask.to(self.device), 
        ).last_hidden_state
        
        return embeddings

    @torch.no_grad()
    def sample(self, texts: List[str],
               sampling_steps: int,
               decoding_steps: int,
               embedding_scale: Optional[float] = 5.0,
               latent_length: Optional[int] = 2048
              ):
        # incease embedding_scale for more text importance, suggested range: 1-15
        # number of output frames = latent_length * num_latent_channels (=32) * compressionfactor (=64) / output_channels (=2)
        
        if type(texts) == str: 
            texts = [texts]
            
        batch_size = len(texts)

        embedding = self.get_text_embeddings(texts).to(self.device)
        noise_latent = torch.randn(batch_size, self.num_latent_channels, latent_length, device=self.device)

        latent = self.diffgen.sample(noise_latent, num_steps=sampling_steps, embedding=embedding, embedding_scale=embedding_scale)
        
        waveform = self.diffae.decode(latent, num_steps=decoding_steps)

        return waveform
        

class DiffGen(pl.LightningModule):
    """ 
    Training wrapper for a textconditional diffusion generator.
    Optionally train on latent space generated by a given diffusion autoencoder. 
    """
    
    def __init__( 
        self,
        lr: float,
        lr_beta1: float,
        lr_beta2: float,
        lr_eps: float,
        lr_weight_decay: float,
        ema_beta: float,
        ema_power: float,
        model: nn.Module,
        text_encoder_name: str,
        text_encoder_max_length: int,
        cfg_mask_prob: Optional[float] = 0.1,
        diffae: Optional[DiffusionAE] = None,
        data_pre_tokenized: Optional[bool] = False,
    ):
        super().__init__()

        # optim params
        self.lr = lr
        self.lr_beta1 = lr_beta1
        self.lr_beta2 = lr_beta2
        self.lr_eps = lr_eps
        self.lr_weight_decay = lr_weight_decay

        # classifier-free guidance: probability of unconditional training
        self.cfg_mask_prob = cfg_mask_prob
        
        # Diffusion Model
        self.model = model
        self.ema = EMA(self.model, beta=ema_beta, power=ema_power)

        # text_encoder
        self.text_encoder_max_length = text_encoder_max_length
        self.text_encoder_name = text_encoder_name

        # tokenizer if needed
        if not data_pre_tokenized:
            self.tokenizer = AutoTokenizer.from_pretrained(self.text_encoder_name, model_max_length=self.text_encoder_max_length)
        else:
            self.tokenizer = None

        # only the encoder part of the transformer in eval mode on the correct device
        self.text_encoder = AutoModel.from_pretrained(text_encoder_name).encoder.to(self.device).eval()

        # Diffusion Autoencoder on the correct device in eval mode if provided 
        self.diffae = diffae.to(self.device).eval() if diffae else None
        
    @property
    def device(self):
        return next(self.parameters()).device

    def configure_optimizers(self):
        optimizer = optim.AdamW(
            list(self.model.parameters()),
            lr=self.lr,
            betas=(self.lr_beta1, self.lr_beta2),
            eps=self.lr_eps,
            weight_decay=self.lr_weight_decay,
        )
        return optimizer
    
    @torch.no_grad()
    def get_text_embeddings(self, labels) -> Tensor:
        # Compute batch of tokens and mask from texts
        if self.tokenizer: 
            tokenized = self.tokenizer.batch_encode_plus(
                labels,
                return_tensors="pt",
                padding="max_length",
                max_length=self.text_encoder_max_length,
                truncation=True,
            )
            ids, mask = tokenized["input_ids"], tokenized["attention_mask"]
        else:
            # data is already tokenized
            ids, mask = labels
        
        # Encode with transformer
        embeddings = self.text_encoder(
            input_ids=ids.to(self.device),
            attention_mask=mask.to(self.device), 
        ).last_hidden_state
        
        return embeddings

    @torch.no_grad()
    def prepare_batch(self, batch):
        waveforms, labels = batch
        embedding = self.get_text_embeddings(labels)
        # Encode with diffae if provided
        latent = self.diffae.encode(waveforms) if self.diffae else waveforms 
        return latent, embedding

    def training_step(self, batch, batch_idx):
        latent, embedding = self.prepare_batch(batch)
        loss = self.model(latent, 
                          embedding=embedding, 
                          embedding_mask_proba=self.cfg_mask_prob,
                         )
        self.log("train_loss", loss)
        
        # Update EMA model and log decay
        self.ema.update()
        self.log("ema_decay", self.ema.get_current_decay())
        return loss

    def validation_step(self, batch, batch_idx):
        latent, embedding = self.prepare_batch(batch)
        loss = self.model(latent, 
                          embedding=embedding, 
                          embedding_mask_proba=self.cfg_mask_prob,
                         )
        self.log("valid_loss", loss)
        return loss


class DMAE(pl.LightningModule):
    """ 
    Training Wrapper for a diffusion (magnitude) autoencoder
    """
    
    def __init__(
        self,
        lr: float,
        lr_eps: float,
        lr_beta1: float,
        lr_beta2: float,
        lr_weight_decay: float,
        ema_beta: float,
        ema_power: float,
        model: DiffusionAE,
    ):
        super().__init__()
        self.lr = lr
        self.lr_eps = lr_eps
        self.lr_beta1 = lr_beta1
        self.lr_beta2 = lr_beta2
        self.lr_weight_decay = lr_weight_decay
        self.model = model
        self.ema = EMA(self.model, beta=ema_beta, power=ema_power)

    @property
    def device(self):
        return next(self.model.parameters()).device

    def configure_optimizers(self):
        optimizer = optim.AdamW(
            list(self.parameters()),
            lr=self.lr,
            betas=(self.lr_beta1, self.lr_beta2),
            eps=self.lr_eps,
            weight_decay=self.lr_weight_decay,
        )
        return optimizer

    def training_step(self, batch, batch_idx):
        waveforms = batch
        loss = self.model(waveforms)
        self.log("train_loss", loss)
        self.ema.update()
        self.log("ema_decay", self.ema.get_current_decay())
        return loss

    def validation_step(self, batch, batch_idx):
        waveforms = batch
        loss = self.ema(waveforms)
        self.log("valid_loss", loss)
        return loss


# a few defaults that help with instantiation
default_text_encoder_max_length = 128
default_diffae_state_dict_file = "/data/logs/ckpts/diffae.pth"
default_diffgen_state_dict_file = "/data/logs/ckpts/diffgen_v3.pth "

# Helperfunctions that help with instantiation
def get_pretrained_diffae_from_huggingface(
    model_path: Optional[str] = "archinetai/dmae1d-ATC64-v2", 
    revision: Optional[str] = "3ffeea6"
):
    """ Note: diffae is pretrained on music data not on Bird sounds"""
    diffae = AutoModel.from_pretrained(
        pretrained_model_name_or_path=model_path,
        trust_remote_code=True,
        revision=revision,
    ).model
    return diffae


def get_default_diffae(state_dict_file: Optional[str] = default_diffae_state_dict_file):
    diffae = get_pretrained_diffae_from_huggingface()
    if state_dict_file is not None:
        diffae.load_state_dict(torch.load(state_dict_file))
    return diffae


def get_default_diffgen(state_dict_file: Optional[str] = default_diffgen_state_dict_file):
    diffgen=DiffusionModel(
        net_t=UNetV0,
        in_channels=32, # U-Net: number of input/output channels
        channels=[128, 256, 512, 512, 1024, 1024], # U-Net: channels at each layer
        factors=[1, 2, 2, 2, 2, 2], # U-Net: downsampling and upsampling factors at each layer
        items=[2, 2, 2, 4, 8, 8], # U-Net: number of repeating items at each layer
        attentions=[0, 0, 1, 1, 1, 1], # U-Net: attention enabled/disabled at each layer
        cross_attentions=[1, 1, 1, 1, 1, 1], # U-Net: cross-attention enabled/disabled at ea
        attention_heads=12, # U-Net: number of attention heads per attention item
        attention_features=64, # U-Net: number of attention features per attention item
        diffusion_t=VDiffusion, # The diffusion method used
        sampler_t=VSampler, # The diffusion sampler used
        embedding_max_length=default_text_encoder_max_length, # U-Net: text embedding maximum length (default for T5-base)
        embedding_features=768, # U-Net: text embedding features (default for T5-base)
        use_text_conditioning=False, # only put true if instead of passing embeddings to model.forward you want to pass raw text 
        use_embedding_cfg=True, # U-Net: enables classifier free guidance
    )
    if state_dict_file is not None:
        diffgen.load_state_dict(torch.load(state_dict_file))
    return diffgen
    

def get_default_combined_sampler(
    diffae_state_dict_file: Optional[str] = default_diffae_state_dict_file,
    diffgen_state_dict_file: Optional[str] = default_diffgen_state_dict_file,
):
    model = CombinedSampler(
        diffae=get_default_diffae(state_dict_file=diffae_state_dict_file),
        diffgen=get_default_diffgen(state_dict_file=diffgen_state_dict_file),
        text_encoder_name="t5-base",
        text_encoder_max_length=default_text_encoder_max_length,
        device="cuda:0",
    )
    return model
